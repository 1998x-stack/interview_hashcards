# BatchNorm

## 1. 引言

Q: BatchNorm旨在解决深度网络中的什么问题？
A: 它旨在解决 **内部协变量偏移** 的问题，即训练过程中层输入的分布发生变化。

Q: 什么是内部协变量偏移？
A: 它是指训练期间由于网络参数更新而导致内部激活值分布发生的变化。

Q: 为什么内部协变量偏移会减缓训练速度？
A: 因为每一层都必须不断适应变化的输入分布，这需要使用更小的学习率和更谨慎的参数初始化。

Q: BatchNorm如何加速训练？
A: 通过在训练期间将层输入归一化，使其具有稳定的均值和方差。

C: 内部协变量偏移指的是训练期间[内部激活值]分布的变化。

C: BatchNorm允许使用[更高]的学习率，并减少对[初始化]的谨慎要求。

---

## 2. 减少内部协变量偏移

Q: 固定层输入的分布有何益处？
A: 它使得各层能在稳定的输入分布上学习，从而提高了收敛速度。

Q: BatchNorm灵感来源于哪种经典技术？
A: 输入白化，该技术强制使数据具有零均值和单位方差。

Q: 为什么对深度网络进行完全的白化不切实际？
A: 因为它需要计算协方差矩阵及其逆矩阵，计算代价高昂且难以微分。

Q: 如果在梯度下降之外应用归一化会发生什么？
A: 参数可能会在损失不变的情况下增长，导致训练不稳定或发散。

C: 白化需要计算[协方差矩阵]及其[逆平方根]。

C: 在反向传播中忽略归一化可能导致参数[爆炸性增长]。

---

## 3. 通过小批量统计量进行归一化

Q: BatchNorm如何简化白化过程？
A: 通过对每个特征独立地进行归一化，而不是联合地对特征进行去相关。

Q: BatchNorm中如何对单个激活值进行归一化？
A: 减去小批量均值，再除以小批量标准差。

Q: 为什么引入可学习的参数 γ 和 β？
A: 为了在归一化后恢复网络的表示能力。

Q: 小批量在BatchNorm中扮演什么角色？
A: 它为训练期间的归一化提供了均值和方差的估计值。

C: BatchNorm将每个特征归一化为零[均值]和单位[方差]。

C: 参数[γ]和[β]允许BatchNorm变换表示[恒等]函数。

---

## 3.1 BatchNorm网络的训练与推断

Q: 为什么在推断时不适用小BatchNorm？
A: 因为推断应该是确定性的，并且应仅依赖于输入，而非其他样本。

Q: 推断时使用什么统计量？
A: 使用从训练数据估计出的总体均值和方差。

Q: 通常如何估计总体统计量？
A: 通过平均小批量统计量或使用移动平均值。

Q: 在推断时，BatchNorm简化为哪种形式？
A: 简化为每个激活值上的固定线性变换。

C: 在推断期间，BatchNorm使用[总体]统计量，而非[小批量]统计量。

C: 推断时的BatchNorm变成了一个单一的[线性]变换。

---

## 3.2 BatchNorm的卷积网络

Q: BatchNorm通常应用于层的哪个位置？
A: 在线性变换（Wu + b）之后，非线性激活函数之前。

Q: 使用BatchNorm时，为什么可以移除偏置项？
A: 因为均值减法抵消了偏置项的效果。

Q: BatchNorm如何适配卷积层？
A: 通过在批次和空间位置上联合地对特征图中的所有激活值进行归一化。

Q: 在卷积BatchNorm中，每个特征图学习哪些参数？
A: 每个特征图学习一个 γ 和一个 β。

C: 在卷积层中，BatchNorm是按[特征图]应用的，而不是按[激活值]。

C: 当使用BatchNorm时，偏置项变得[冗余]。

---

## 3.3 BatchNorm支持更高的学习率

Q: 为什么高学习率通常会使深度网络不稳定？
A: 因为它们放大了参数更新，导致梯度爆炸或消失。

Q: BatchNorm如何在使用高学习率时稳定训练？
A: 通过使梯度传播对参数尺度保持不变。

Q: BatchNorm对梯度流有什么影响？
A: 它有助于在整个网络中保持良好的梯度。

Q: BatchNorm对雅可比矩阵奇异值有何推测影响？
A: 它们倾向于接近 1，这保留了梯度大小。

C: BatchNorm使训练对[参数尺度]更加鲁棒。

C: BatchNorm有助于防止梯度[爆炸]或[消失]。

---

## 3.4 BatchNorm对模型的正则化作用

Q: 为什么BatchNorm起到正则化的作用？
A: 因为每个样本是相对于小批量中的其他样本进行归一化的。

Q: BatchNorm对 Dropout 的需求有何影响？
A: 它通常减少或消除对 Dropout 的需求。

Q: 为什么BatchNorm在训练时会引入噪声？
A: 因为小批量统计量在不同批次之间会发生变化。

C: BatchNorm通过小批量统计量引入了[随机性]。

C: BatchNorm可以减少对[Dropout]作为正则化器的需求。