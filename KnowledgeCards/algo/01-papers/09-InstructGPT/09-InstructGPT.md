# InstructGPT 论文记忆卡片

## 第一章：核心动机

Q: InstructGPT 要解决什么问题？
A: 使语言模型与用户意图对齐（alignment），让模型更有帮助、诚实、无害（helpful, honest, harmless）

C: 传统语言模型目标是 [预测下一个词]，但这与 [遵循用户指令] 的目标不一致

Q: 为什么大模型不一定更好？
A: 模型变大不等于更好地遵循用户意图，可能产生不真实、有毒或无用的输出

C: InstructGPT 使用 [人类反馈强化学习]（RLHF）来对齐模型与人类意图

---

## 第二章：三步训练方法

Q: InstructGPT 的训练包括哪三个步骤？
A: 1) 监督微调（SFT）；2) 奖励模型训练（RM）；3) 强化学习优化（PPO）

C: Step 1 [监督微调]：用标注员编写的示范数据训练模型

C: Step 2 [奖励模型]：训练一个模型预测人类偏好的输出

C: Step 3 [PPO]：用奖励模型作为奖励函数，通过 PPO 算法优化策略

Q: 为什么需要三个步骤而不是直接监督学习？
A: 监督学习数据有限且昂贵，通过强化学习可以利用更多的提示数据，并持续优化

---

## 第三章：监督微调（SFT）

Q: SFT 阶段使用什么数据？
A: 标注员编写的高质量示范（demonstrations），展示模型应该如何响应指令

C: SFT 训练 [16 epochs]，使用 [cosine] 学习率衰减

Q: 为什么 SFT 模型会过拟合验证损失但仍继续训练？
A: 虽然验证损失上升，但奖励模型评分和人类偏好评分继续提升

Q: SFT 的数据来源有哪些？
A: 1) 标注员编写的提示；2) OpenAI API 用户提交的提示（早期 InstructGPT 版本）

---

## 第四章：奖励模型（RM）

Q: 奖励模型的作用是什么？
A: 预测人类更偏好哪个输出，输出标量奖励分数

C: 奖励模型从 [SFT 模型] 初始化，去掉 [unembedding 层]，增加一个输出标量的头

Q: 奖励模型的训练数据是什么？
A: 对同一提示的多个输出的排序比较，标注员标注哪个输出更好

Q: 奖励模型的损失函数是什么？
A: 交叉熵损失，基于成对比较：loss = -E[log(σ(r_θ(x,y_w) - r_θ(x,y_l)))]

C: 对于 K 个输出，可以得到 [C(K,2)] 个成对比较

Q: 为什么使用 6B 而不是 175B 的奖励模型？
A: 6B 模型训练更稳定，且作为价值函数使用时效果足够好，节省计算

---

## 第五章：PPO 强化学习

Q: PPO 阶段的目标函数是什么？
A: objective = E[(r_θ(x,y) - β·KL(π_RL || π_SFT)) + γ·log(π_RL(x_pretrain))]

C: PPO 目标包含三项：[奖励模型得分] - [KL 惩罚] + [预训练数据混合]

Q: KL 惩罚的作用是什么？
A: 防止模型偏离初始策略太远，避免过度优化奖励模型导致的异常输出

C: KL 惩罚系数 β 默认为 [0.02]

Q: 为什么要混入预训练梯度（PPO-ptx）？
A: 缓解在公共 NLP 数据集上的性能退化（alignment tax），保持模型原有能力

C: 预训练混合系数 γ 为 [27.8]，预训练数据是 GPT-3 训练数据的随机样本

---

## 第六章：数据集构建

Q: InstructGPT 数据集的主要来源是什么？
A: 1) 标注员编写的提示；2) OpenAI API Playground 用户提交的提示

C: 数据集包含三种提示类型：[Plain]（任意任务）、[Few-shot]（带示例）、[User-based]（基于真实用例）

Q: 数据集的大小是多少？
A: SFT: 13k 提示；RM: 33k 提示；PPO: 31k 提示

Q: 如何确保数据隐私？
A: 过滤包含个人身份信息（PII）的提示，且只使用 Playground 数据（有明确告知）

C: 数据集约 [96%] 是英语，其他语言占比很小

---

## 第七章：评估指标

Q: InstructGPT 的主要评估方式是什么？
A: 人类评估：标注员对模型输出进行偏好排序和 Likert 量表评分（1-7分）

C: 评估维度包括：[有帮助]（helpful）、[诚实]（honest）、[无害]（harmless）

Q: 如何评估模型的真实性？
A: 使用 TruthfulQA 数据集，测量生成真实且信息性答案的频率

Q: 如何评估模型的毒性？
A: 使用 RealToxicityPrompts 数据集，通过 Perspective API 和人工评估毒性

C: 偏见评估使用 [Winogender] 和 [CrowS-Pairs] 数据集

---

## 第八章：主要结果

Q: 1.3B InstructGPT 相比 175B GPT-3 表现如何？
A: 人类评估中，1.3B InstructGPT 的输出被优先选择，尽管参数少 100 倍

C: 175B InstructGPT 的输出在 [85±3%] 的情况下优于 175B GPT-3

Q: InstructGPT 在真实性上的提升？
A: 在 TruthfulQA 上，生成真实且信息性答案的频率是 GPT-3 的两倍

Q: InstructGPT 在毒性上的表现？
A: 被要求尊重时，生成毒性输出减少约 25%，但在偏见上改进不明显

C: InstructGPT 在公共 NLP 数据集上的性能退化可通过 [PPO-ptx] 缓解

---

## 第九章：泛化能力

Q: InstructGPT 能否泛化到非英语指令？
A: 可以，尽管训练数据几乎全是英语，模型能遵循其他语言的指令（如法语、西班牙语）

Q: InstructGPT 能处理代码相关任务吗？
A: 可以，能理解和回答关于代码的问题，尽管代码在微调数据中占比很小

C: InstructGPT 展示了将 [遵循指令] 这一概念泛化到训练分布之外的能力

---

## 第十章：局限性

Q: InstructGPT 仍然存在哪些问题？
A: 1) 仍会犯简单错误；2) 可能虚构信息；3) 对包含错误前提的指令容易被误导；4) 倾向于过度避险

C: InstructGPT 在 [假前提指令] 上容易被误导，会接受错误前提

Q: "对齐税"是什么？
A: 为了对齐而导致的性能退化，特别是在某些公共 NLP 任务上

Q: InstructGPT 的安全性问题？
A: 如果用户明确要求有害输出，模型仍会遵循指令，因为训练优先考虑有用性

---

## 第十一章：标注员选择

Q: 如何选择标注员？
A: 通过筛选测试，选择擅长识别敏感内容、与研究员偏好一致的标注员

C: 标注员团队约 [40 人]，主要通过 [Upwork] 和 [Scale AI] 招募

Q: 标注员间的一致性如何？
A: 训练标注员之间的一致率为 72.6±1.5%，held-out 标注员为 77.3±1.3%

Q: 标注指南的核心原则是什么？
A: 优先考虑真实性和无害性，而非单纯的有用性（在最终评估中）

---

## 第十二章：对齐到谁？

Q: InstructGPT 对齐的是谁的价值观？
A: 主要是标注员和研究人员的偏好，而非所有人的价值观

C: "对齐"的目标对象包括：[标注员]、[研究人员]、[API 客户]

Q: 如何处理不同群体间的偏好差异？
A: 这是开放问题，论文建议训练可以根据不同群体偏好条件化的模型

Q: 为什么不能对齐所有人的价值观？
A: 不可能训练一个同时对齐所有人偏好的系统，不同群体可能有冲突的价值观

---

## 第十三章：技术细节

Q: 所有模型使用什么架构？
A: GPT-3 架构，使用 fp16 权重和激活，fp32 主副本

C: 上下文长度为 [2k tokens]，过滤超过 1k tokens 的提示

Q: SFT 的超参数设置？
A: 16 epochs, 学习率 9.65e-6 (1.3B/6B) 或 5.03e-6 (175B)，batch size 32 或 8

Q: PPO 的训练细节？
A: 256k episodes，batch size 512，minibatch 64，恒定学习率加 warmup

C: 奖励模型训练 [1 epoch]，batch size [64]，学习率 [9e-6]

---

## 第十四章：对比实验

Q: FLAN 和 T0 模型与 InstructGPT 相比如何？
A: 在 API 提示分布上表现不如 InstructGPT，说明公共 NLP 数据集不够多样化

C: InstructGPT 相比 FLAN 的赢率为 [78±4%]，相比 T0 为 [79±4%]

Q: 为什么公共 NLP 数据集不够？
A: 这些数据集主要是分类和问答任务，而真实用例中生成和头脑风暴占主导（57%）

---

## 第十五章：alignment 研究启示

Q: 这项工作对 alignment 研究的启示？
A: 1) 对齐成本低；2) 模型能泛化指令遵循；3) alignment tax 可以缓解；4) 在真实产品中验证技术

C: 对齐成本相对于预训练是 [modest]（适中的），值得投资

Q: 什么是"对齐税"的缓解？
A: 通过 PPO-ptx（混入预训练数据），在保持对齐的同时恢复原有能力

Q: 未来研究方向有哪些？
A: 1) 对抗数据收集；2) 拒绝有害指令；3) 结合其他可控性方法；4) 更好的评估指标

---

## 第十六章：公式总结

C: 奖励模型损失：loss = -[1/C(K,2)] · E[log σ([r_θ(x,y_w)] - [r_θ(x,y_l)])]

C: PPO 目标：E[[r_θ(x,y)] - [β·log(π_RL/π_SFT)] + [γ·log π_RL(x_pretrain)]]

Q: 为什么奖励模型要归一化？
A: 让示范数据的平均奖励为 0，作为 RL 的起点

---

## 第十七章：实验设置

Q: 如何划分训练/验证/测试集？
A: 按用户 ID 划分，确保验证/测试集不包含训练集用户的数据

Q: 如何去重提示？
A: 启发式检查共享长公共前缀的提示，限制每用户约 200 个提示

C: 数据集分类包括：[生成]（45.6%）、[开放QA]（12.4%）、[头脑风暴]（11.2%）

---

## 第十八章：broader impacts

Q: InstructGPT 的潜在积极影响？
A: 减少语言模型的有害输出，使模型更有用、更真实

Q: InstructGPT 的潜在负面影响？
A: 更容易被滥用（生成误导信息），可能强化特定群体的价值观而非普遍价值

C: 部署模型需要考虑：[用例限制]、[监控滥用]、[速率限制]

Q: 如何防止恶意使用？
A: API 监控、撤销滥用者访问权限、限制某些应用场景

---

## 对比表：模型演进

| 模型 | 训练方法 | 数据来源 | 主要特点 |
|------|---------|----------|---------|
| GPT-3 | 预训练 | 网络文本 | 强大但不对齐 |
| SFT | 监督微调 | 人类示范 | 模仿人类 |
| InstructGPT (PPO) | RLHF | 人类偏好排序 | 对齐人类意图 |
| InstructGPT (PPO-ptx) | RLHF + 预训练混合 | 偏好 + 预训练 | 对齐且保持能力 |

---

## 代码实现关键点

```python
# 奖励模型训练伪代码
for batch in comparison_data:
    # batch 包含 K 个输出的排序
    rewards = reward_model(batch)
    # 计算所有成对比较的损失
    loss = -log(sigmoid(rewards[better] - rewards[worse]))
    loss.backward()

# PPO 训练伪代码  
for batch in rollout_data:
    # 计算奖励
    reward = reward_model(state, action) - kl_penalty
    # PPO clip 更新
    advantage = reward - value_function(state)
    policy_loss = -min(ratio * advantage, 
                       clip(ratio, 1-ε, 1+ε) * advantage)
    # 混入预训练梯度
    pretrain_loss = -log_prob(pretrain_data)
    total_loss = policy_loss + γ * pretrain_loss
```

C: RLHF 的关键是将 [人类偏好] 转换为 [奖励信号]

---

## 关键 takeaways

Q: InstructGPT 最重要的贡献是什么？
A: 展示了 RLHF 可以有效地让大语言模型与人类意图对齐，且成本合理

C: InstructGPT 证明：[小模型 + 对齐] > [大模型 - 对齐]

Q: 如何在自己的项目中应用这些技术？
A: 1) 收集高质量示范；2) 训练奖励模型；3) 用 PPO 优化；4) 持续迭代

最终结论：对齐是值得投资的，能显著提升模型的实用性和安全性。