# 🤖 大语言模型面试 Hashcards

> 按照 hashcards 格式设计，使用 `Q:` 和 `A:` 标记问答，`[关键词]` 用于挖空记忆

---

## 一、LLM 基础

### 1.1 发展历程

Q: 语言模型的核心任务是什么？
A: 建模文本序列的[概率分布]：$P(w_1, w_2, ..., w_n) = \prod P(w_i | w_1, ..., w_{i-1})$，即给定前文预测下一个词。

Q: 语言模型的发展路线？
A: N-gram → RNN/LSTM → [Transformer]（2017）→ GPT/BERT（2018）→ GPT-3（2020）→ ChatGPT/GPT-4（2022-2023）。

Q: 大语言模型"大"体现在哪？
A: ①[参数量]大（数十亿到万亿）；②[训练数据]大（TB 级）；③[计算资源]大（千卡级别）。

Q: LLM 的涌现能力（Emergent Abilities）是什么？
A: 当模型规模超过某个阈值后，突然出现的能力（如思维链、上下文学习），小模型不具备。与模型规模呈[非线性]关系。

### 1.2 分词与词向量

Q: 常见的分词方法有哪些？
A: ①基于词典：正向/逆向最大匹配；②基于统计：[BPE]、[WordPiece]、[SentencePiece]；③基于深度学习：端到端分词。

Q: BPE（Byte Pair Encoding）的原理？
A: ①初始化为字符级词表；②统计相邻 token 对频率；③合并[最高频]的 pair 为新 token；④重复直到达到词表大小。

Q: WordPiece 和 BPE 的区别？
A: BPE 按[频率]合并。WordPiece 按[似然增益]合并：选择使语言模型概率提升最大的 pair。BERT 使用 WordPiece。

Q: 为什么 LLM 使用子词分词而非词级分词？
A: ①处理 [OOV]（未登录词）问题；②减小词表大小；③跨语言共享词表；④保留形态学信息。

Q: Word2Vec 的两种训练方式？
A: ①[CBOW]：用上下文预测中心词；②[Skip-gram]：用中心词预测上下文。Skip-gram 对低频词效果更好。

Q: Word2Vec 的负采样是什么？
A: 将多分类问题转为二分类：正样本（真实上下文词）+ [负采样]若干非上下文词，计算效率大幅提升。

---

## 二、Transformer 架构

### 2.1 整体架构

Q: Transformer 的核心组件？
A: ①[Multi-Head Attention]；②[Feed-Forward Network]；③[Layer Normalization]；④[Residual Connection]；⑤Positional Encoding。

Q: Encoder-only、Decoder-only、Encoder-Decoder 的代表模型？
A: [Encoder-only]：BERT（理解任务）。[Decoder-only]：GPT 系列（生成任务）。[Encoder-Decoder]：T5、BART（Seq2Seq 任务）。

Q: 为什么现在主流 LLM 都是 Decoder-only 架构？
A: ①更适合[自回归]生成；②训练目标统一（next token prediction）；③[涌现能力]更强；④Zero-shot/Few-shot 能力更好；⑤工程实现简单。

### 2.2 注意力机制

Q: Self-Attention 的计算公式？
A: $Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$。Q、K、V 分别是查询、键、值矩阵，$\sqrt{d_k}$ 用于[缩放]防止梯度消失。

Q: 为什么要除以 $\sqrt{d_k}$？
A: 当 $d_k$ 较大时，$QK^T$ 的值较大，softmax 进入[饱和区]，梯度很小。除以 $\sqrt{d_k}$ 使方差保持在 1 附近。

Q: Multi-Head Attention 的作用？
A: 不同的头关注[不同子空间]的信息，学习不同类型的关联（如句法、语义）。$MultiHead = Concat(head_1,...,head_h)W^O$。

Q: MHA、MQA、GQA 的区别？
A: [MHA]：每个头独立的 K、V。[MQA]：所有头共享 K、V，推理快。[GQA]：分组共享 K、V，平衡性能和效率。

Q: Causal/Masked Attention 是什么？
A: Decoder 中使用的[因果注意力]：当前位置只能看到之前的 token，通过下三角 mask 实现，防止信息泄露。

### 2.3 位置编码

Q: 为什么 Transformer 需要位置编码？
A: Self-Attention 是[置换不变]的（无视位置顺序），必须通过位置编码注入位置信息。

Q: Sinusoidal 位置编码的特点？
A: 用不同频率的正余弦函数编码位置，可以处理[任意长度]序列，且相对位置关系可以通过线性变换表示。

Q: RoPE（Rotary Position Embedding）的原理？
A: 将位置编码与注意力计算[融合]：对 Q、K 向量做旋转变换，使点积只依赖于相对位置。LLaMA、Qwen 等使用。

Q: RoPE 的优势？
A: ①[相对位置]编码更自然；②外推能力强；③可与 Flash Attention 结合；④计算效率高。

Q: ALiBi（Attention with Linear Biases）的思想？
A: 不修改 Q、K，而是在注意力分数上加[线性偏置]惩罚远距离 token，实现长度外推。

### 2.4 Layer Normalization

Q: Layer Norm 的计算过程？
A: 对单个样本的[所有特征]做归一化：$\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} \cdot \gamma + \beta$，不依赖 batch size。

Q: Pre-Norm 和 Post-Norm 的区别？
A: [Pre-Norm]：LN 在 Attention/FFN 之前，训练更稳定，LLaMA 使用。[Post-Norm]：LN 在残差之后，原始 Transformer 设计。

Q: RMSNorm 相比 LayerNorm 的改进？
A: 只用均方根归一化（去掉减均值）：$\hat{x} = \frac{x}{\sqrt{\frac{1}{n}\sum x_i^2 + \epsilon}}$，计算更简单，效果相当。LLaMA 使用。

### 2.5 FFN 与激活函数

Q: Transformer 中 FFN 的结构？
A: 两层线性变换 + 激活函数：$FFN(x) = W_2 \cdot \sigma(W_1 x + b_1) + b_2$，中间维度通常是 [4 倍]隐藏层维度。

Q: GLU（Gated Linear Unit）变体的作用？
A: $GLU(x) = (W_1 x) \otimes \sigma(W_2 x)$，引入[门控机制]。SwiGLU、GeGLU 在 LLM 中表现更好，PaLM、LLaMA 使用。

Q: SwiGLU 的公式？
A: $SwiGLU(x) = Swish(W_1 x) \otimes (W_2 x)$，结合了 [Swish 激活]和门控，参数量增加但效果更好。

---

## 三、训练与微调

### 3.1 预训练

Q: LLM 预训练的主要目标？
A: [Causal Language Modeling]：预测下一个 token，$L = -\sum \log P(x_t | x_{<t})$。BERT 用 Masked LM，GPT 用 CLM。

Q: 预训练数据的处理流程？
A: 数据收集 → [清洗去重] → 质量过滤 → 分词 → 打包成固定长度序列 → 构建训练集。

Q: 常见的预训练数据来源？
A: ①网页爬取（Common Crawl）；②书籍（Books3）；③代码（GitHub）；④百科（Wikipedia）；⑤论文（arXiv）。

Q: LLM 训练的计算量估算？
A: FLOPs ≈ [6 × 参数量 × Token 数]。根据 Chinchilla 定律，最优：Token 数 ≈ 20 × 参数量。

### 3.2 Prompt Engineering

Q: Prompt 的组成要素？
A: ①[指令]（任务描述）；②[上下文]（背景信息）；③[示例]（Few-shot 样例）；④[输入]（待处理内容）；⑤[输出格式]。

Q: Zero-shot、Few-shot、Chain-of-Thought 的区别？
A: [Zero-shot]：直接给指令。[Few-shot]：提供示例。[CoT]：引导模型展示推理过程（"Let's think step by step"）。

Q: Chain-of-Thought 为什么有效？
A: ①将复杂问题[分解]为子步骤；②中间结果作为工作记忆；③减少推理跳跃带来的错误。

### 3.3 微调方法

Q: 全量微调的问题？
A: ①需要存储完整模型副本，[显存开销]大；②容易过拟合到下游任务；③[灾难性遗忘]预训练知识。

Q: LoRA 的核心思想？
A: 冻结原始权重，用低秩分解增量更新：$W' = W + \Delta W = W + BA$，其中 B、A 是[低秩]矩阵，参数量大幅减少。

Q: LoRA 的数学表达？
A: $h = (W + BA)x$，其中 $B \in \mathbb{R}^{d \times r}$，$A \in \mathbb{R}^{r \times d}$，$r \ll d$。只训练 A、B。

Q: QLoRA 的改进？
A: ①基础模型 [4-bit 量化]；②LoRA 适配器用 16-bit；③分页优化器处理显存峰值。在消费级 GPU 上微调大模型。

Q: Adapter Tuning 的原理？
A: 在 Transformer 层中插入小型 [Adapter 模块]（下采样→非线性→上采样），只训练 Adapter，原模型冻结。

Q: P-Tuning 和 Prefix-Tuning 的区别？
A: [P-Tuning]：优化输入端的连续 prompt 向量。[Prefix-Tuning]：在每层的 K、V 前加可学习前缀。

---

## 四、分布式训练

### 4.1 并行策略

Q: 数据并行（DP）的原理？
A: 每个 GPU 持有完整模型副本，处理不同数据，梯度 [AllReduce] 聚合后同步更新。通信量 = 参数量。

Q: ZeRO 优化的三个阶段？
A: [ZeRO-1]：划分优化器状态。[ZeRO-2]：+ 划分梯度。[ZeRO-3]：+ 划分模型参数。逐步减少显存占用。

Q: 流水线并行（PP）的思想？
A: 将模型按层[切分]到不同 GPU，数据流水线传递。问题：bubble 时间（空闲等待）。解决：micro-batch。

Q: 张量并行（TP）的原理？
A: 将单层的矩阵计算[切分]到多个 GPU：列切分（Megatron-LM）或行切分。通信频繁但显存效率高。

Q: 3D 并行是什么？
A: 组合使用 [DP + PP + TP]：TP 在节点内（高带宽），PP 跨节点，DP 扩展规模。如 GPT-3 的训练配置。

### 4.2 训练框架

Q: DeepSpeed 的核心特性？
A: ①[ZeRO] 系列优化；②混合精度训练；③Offload（CPU/NVMe）；④Pipeline 并行；⑤稀疏注意力。

Q: Megatron-LM 的特点？
A: NVIDIA 开发，专注 [张量并行]，高效的 Transformer 实现，支持大规模模型训练。

### 4.3 显存优化

Q: LLM 训练显存占用的组成？
A: ①模型参数；②[优化器状态]（Adam 需 2x 参数）；③梯度；④激活值（与序列长度、batch 相关）。

Q: 混合精度训练的原理？
A: 前向/反向用 [FP16/BF16]（快、省显存），权重主副本和梯度累积用 FP32（精度）。Loss scaling 防止下溢。

Q: BF16 相比 FP16 的优势？
A: [BF16]：指数位多（8 bit），[动态范围]大，不易溢出，不需要 loss scaling。FP16 精度更高但范围小。

Q: Gradient Checkpointing 的原理？
A: 前向时只保存[部分激活]，反向时重计算丢弃的激活。用计算换显存，节省 ~60% 激活显存。

Q: Flash Attention 的核心思想？
A: 通过 [分块计算]和 [kernel 融合]，减少 HBM 访问次数，显著提升 Attention 计算效率和显存使用。

---

## 五、推理优化

### 5.1 推理框架

Q: LLM 推理的两个阶段？
A: ①[Prefill]：处理输入 prompt，计算 KV Cache，可并行。②[Decode]：逐 token 生成，[自回归]，串行。

Q: KV Cache 的作用？
A: 缓存已计算的 Key、Value 矩阵，避免重复计算。Decode 阶段每步只需计算[新 token]的 K、V。

Q: KV Cache 的显存计算？
A: $2 \times layers \times hidden \times seq\_len \times batch \times precision$（K 和 V 各一份）。长序列时占用巨大。

Q: vLLM 的核心技术 PagedAttention？
A: 将 KV Cache 分成固定大小的 [块]（Pages），按需分配和释放，避免显存碎片，支持动态 batch。

Q: Continuous Batching 的思想？
A: 请求完成立即释放，新请求立即加入，而非等待整个 batch 完成。提升 [吞吐量] 和 GPU 利用率。

### 5.2 解码策略

Q: Greedy Decoding 的问题？
A: 每步选概率最大的 token，容易陷入[重复]，缺乏多样性，可能错过全局最优。

Q: Beam Search 的原理？
A: 维护 k 个最优序列（beam），每步扩展后保留 top-k。权衡[质量和多样性]，常用于翻译。

Q: Top-k Sampling 的原理？
A: 只从概率最高的 [k 个] token 中采样，过滤低概率 token。k 小→更集中，k 大→更多样。

Q: Top-p (Nucleus) Sampling 的原理？
A: 选择累积概率达到 [p] 的最小 token 集合进行采样。自适应，概率分布集中时集合小，分散时集合大。

Q: Temperature 的作用？
A: 调节 softmax 的平滑度：$P(x_i) = \frac{e^{z_i/T}}{\sum e^{z_j/T}}$。[T<1]：更尖锐（确定）。[T>1]：更平滑（随机）。

### 5.3 量化

Q: 模型量化的目的？
A: 将 FP32/FP16 权重转为 [INT8/INT4]，减少模型大小和推理显存，提升吞吐量，可能轻微影响精度。

Q: PTQ 和 QAT 的区别？
A: [PTQ]（Post-Training Quantization）：训练后量化，简单快速。[QAT]（Quantization-Aware Training）：训练中模拟量化，精度更好。

Q: GPTQ 的原理？
A: 基于二阶信息（Hessian）的 PTQ 方法，按列量化并用剩余列[补偿误差]，4-bit 量化精度损失小。

Q: AWQ（Activation-aware Weight Quantization）的思想？
A: 根据[激活分布]确定权重重要性，重要权重保持高精度，不重要的激活量化。优于等权重处理。

---

## 六、对齐与 RLHF

### 6.1 RLHF 流程

Q: RLHF 的三个阶段？
A: ①[SFT]：监督微调，学习对话格式；②[RM]：训练奖励模型，学习人类偏好；③[PPO]：用 RL 优化策略最大化奖励。

Q: 奖励模型（RM）的训练方式？
A: 给定 prompt，人类对多个回答[排序]，RM 学习预测人类偏好：$L = -\log\sigma(r_w - r_l)$（Bradley-Terry 模型）。

Q: RLHF 中 PPO 的目标函数？
A: $J = \mathbb{E}[R(x,y) - \beta \cdot KL(\pi || \pi_{ref})]$，最大化奖励同时用 [KL 惩罚]保持与参考模型接近。

Q: 为什么需要 KL 惩罚？
A: 防止策略偏离 SFT 模型太远，避免：①[奖励 hacking]（找到高奖励但无意义的输出）；②失去语言能力；③过拟合奖励模型。

### 6.2 DPO 及变体

Q: DPO 的核心思想？
A: 将 RL 目标重参数化，直接用偏好数据优化策略，[无需训练奖励模型]：$L = -\log\sigma(\beta(r_w - r_l))$，其中 r 由策略隐式定义。

Q: DPO 相比 RLHF 的优势？
A: ①[简单]：无需奖励模型和 RL 训练；②[稳定]：普通监督学习；③高效：计算成本低。

Q: DPO 的潜在问题？
A: ①依赖高质量偏好数据；②可能过拟合选定的偏好；③某些场景效果不如 RLHF；④[分布偏移]问题。

Q: IPO、KTO 等 DPO 变体的改进方向？
A: [IPO]：解决 DPO 过拟合。[KTO]：不需要配对数据。[ORPO]：统一 SFT 和偏好优化。

---

## 七、RAG 与 Agent

### 7.1 RAG

Q: RAG 的核心思想？
A: [检索增强生成]：先从知识库检索相关文档，将检索结果作为上下文，再让 LLM 生成回答。弥补 LLM 知识时效性和幻觉问题。

Q: RAG 的基本流程？
A: ①Query 处理 → ②向量化 → ③[检索]相似文档 → ④Rerank 排序 → ⑤上下文组装 → ⑥LLM [生成]。

Q: RAG 常用的检索方法？
A: ①[稀疏检索]：BM25、TF-IDF；②[稠密检索]：向量相似度（FAISS）；③[混合检索]：结合两者。

Q: RAG 的主要挑战？
A: ①检索质量影响生成；②[上下文窗口]限制；③检索延迟；④知识冲突处理；⑤多跳推理困难。

Q: 如何提升 RAG 效果？
A: ①优化 [Chunk] 策略；②Query 重写/扩展；③Reranker 精排；④Self-RAG（选择性检索）；⑤多轮检索。

### 7.2 Agent

Q: LLM Agent 的核心组件？
A: ①[LLM]：核心推理；②[工具]（Tools）：扩展能力；③[记忆]（Memory）：上下文管理；④[规划]（Planning）：任务分解。

Q: ReAct 框架的思想？
A: [Reasoning + Acting]：LLM 交替进行推理（思考）和行动（调用工具），推理指导行动，行动结果更新推理。

Q: Function Calling 的实现方式？
A: ①定义函数 schema（名称、参数、描述）；②LLM 判断何时调用、生成参数；③执行函数；④将结果返回 LLM。

Q: Agent 的记忆类型？
A: ①[短期记忆]：当前对话上下文；②[长期记忆]：向量数据库存储历史；③[工作记忆]：任务执行中间状态。

---

## 八、模型评估与幻觉

### 8.1 评估方法

Q: LLM 评估的主要维度？
A: ①[能力评估]：知识、推理、代码、数学；②[对齐评估]：安全性、有用性；③[效率评估]：延迟、吞吐、成本。

Q: 常用的 LLM 评估基准？
A: 知识：[MMLU]、TriviaQA。推理：[GSM8K]、BBH。代码：[HumanEval]、MBPP。综合：[HELM]、AlpacaEval。

Q: LLM 评估面临的挑战？
A: ①[数据污染]：训练集包含测试题；②评估饱和；③主观任务难量化；④基准与真实能力差距。

### 8.2 幻觉问题

Q: LLM 幻觉的定义？
A: 模型生成看似合理但[事实错误]或与输入/上下文矛盾的内容。包括：事实性幻觉、忠实性幻觉。

Q: 幻觉产生的原因？
A: ①训练数据[噪声和偏差]；②解码时的随机性；③知识边界模糊；④过度泛化；⑤输入歧义。

Q: 缓解幻觉的方法？
A: ①[RAG]：引入外部知识；②RLHF：对齐人类偏好；③Self-consistency：多次采样投票；④Grounding：明确引用来源；⑤不确定性表达。

---

## 九、高效架构

### 9.1 MoE

Q: MoE（Mixture of Experts）的核心思想？
A: 用多个 [专家网络]（FFN）替代单个 FFN，通过 [Router/Gate] 选择性激活部分专家，增加参数量但保持计算量。

Q: MoE 的优势？
A: ①增大模型容量但[计算量不变]；②不同专家可专注不同知识；③训练和推理更高效。

Q: MoE 的典型配置？
A: 如 Mixtral 8×7B：8 个专家，每次激活 2 个，总参数 47B，激活参数 ~13B。实现"大模型，小成本"。

Q: MoE 训练的挑战？
A: ①[负载均衡]：专家被调用不均匀；②路由不稳定；③Expert Collapse（专家坍缩）；④通信开销。

### 9.2 长上下文

Q: 处理长上下文的主要方法？
A: ①位置编码外推（RoPE + [NTK]、YaRN）；②[稀疏注意力]；③[滑动窗口]注意力；④压缩/摘要上下文；⑤递归处理。

Q: Longformer 的稀疏注意力设计？
A: 结合[局部滑动窗口]（捕获局部关系）+ [全局注意力]（特殊 token 全局可见），复杂度从 $O(n^2)$ 降到 $O(n)$。

Q: Ring Attention 的思想？
A: 将序列[分块]分布到多个 GPU，通过环形通信传递 KV，实现超长序列处理，理论上无限长度。

---

## 十、前沿方向

### 10.1 推理能力

Q: LLM 推理的主要提升方法？
A: ①[思维链] CoT；②Self-Consistency（多次采样投票）；③Tree-of-Thoughts；④[Process Reward Model]（过程奖励）。

Q: o1/o3 模型的核心改进？
A: 测试时[扩展计算]：通过更长的推理链、更多的搜索步骤来提升复杂问题的推理能力，本质是用推理时间换准确率。

### 10.2 多模态

Q: 多模态 LLM 的典型架构？
A: ①[视觉编码器]（如 CLIP ViT）提取图像特征；②[投影层]将图像特征对齐到 LLM 空间；③[LLM] 处理文本和图像 token。

Q: LLaVA 的训练策略？
A: ①[预训练]：冻结视觉和 LLM，只训练投影层（图文对齐）；②[指令微调]：端到端训练（视觉问答数据）。

### 10.3 高效微调

Q: 当前参数高效微调的主流方法？
A: [LoRA] 及其变体最广泛使用。其他：Adapter、Prefix-Tuning、P-Tuning、BitFit（只调偏置）。

Q: LoRA 的典型 rank 设置？
A: 常用 [r=8~64]。r 越大表达能力越强但参数越多。任务简单用小 r，复杂任务用大 r。

---

*Generated by hashcards format for spaced repetition learning*