# 📗 强化学习面试 Hashcards

> 按照 hashcards 格式设计，使用 `Q:` 和 `A:` 标记问答，`[关键词]` 用于挖空记忆

---

## 一、强化学习基础

### 1.1 基本概念

Q: 强化学习解决什么样的问题？
A: 智能体（Agent）在与[环境]的交互中，通过试错学习最优[策略]，以最大化长期累积奖励。核心要素：状态、动作、奖励、策略。

Q: 强化学习与监督学习的主要区别？
A: ①监督学习有[标签]，RL 只有奖励信号；②RL 数据[非独立同分布]，有时序依赖；③RL 存在[探索-利用]权衡；④RL 有延迟奖励问题。

Q: 强化学习的核心要素是什么？
A: [状态 S]：环境的描述；[动作 A]：智能体的决策；[奖励 R]：即时反馈；[策略 π]：状态到动作的映射；[转移概率 P]：状态转移动态。

Q: 强化学习的目标函数是什么？
A: 最大化[期望累积折扣奖励]：$J(\pi) = \mathbb{E}[\sum_{t=0}^{\infty} \gamma^t r_t]$，其中 γ ∈ [0,1) 是折扣因子。

Q: 折扣因子 γ 的作用？
A: ①保证无限时间步的回报[有界]；②反映对未来奖励的重视程度；③γ 大→更关注长期，γ 小→更关注短期。

### 1.2 马尔可夫决策过程 (MDP)

Q: MDP 的马尔可夫性是什么？
A: 下一状态只依赖于当前状态和动作，与历史无关：$P(s_{t+1}|s_t, a_t, s_{t-1}, ...) = P(s_{t+1}|s_t, a_t)$，即[无后效性]。

Q: MDP 的五元组是什么？
A: $(S, A, P, R, \gamma)$：[状态空间]、[动作空间]、[转移概率]、[奖励函数]、[折扣因子]。

Q: 什么是状态值函数 V(s)？
A: 从状态 s 出发，遵循策略 π 的[期望累积回报]：$V^\pi(s) = \mathbb{E}_\pi[\sum_{t=0}^{\infty} \gamma^t r_t | s_0 = s]$。

Q: 什么是动作值函数 Q(s,a)？
A: 从状态 s 执行动作 a 后，遵循策略 π 的[期望累积回报]：$Q^\pi(s,a) = \mathbb{E}_\pi[\sum_{t=0}^{\infty} \gamma^t r_t | s_0 = s, a_0 = a]$。

Q: V 和 Q 的关系？
A: $V^\pi(s) = \sum_a \pi(a|s) Q^\pi(s,a)$（按策略[加权]），或 $V^\pi(s) = \max_a Q^\pi(s,a)$（最优策略下）。

### 1.3 贝尔曼方程

Q: 写出状态值函数的贝尔曼方程。
A: $V^\pi(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a)[R(s,a,s') + \gamma V^\pi(s')]$，表达了当前值与后继值的[递归关系]。

Q: 写出动作值函数的贝尔曼方程。
A: $Q^\pi(s,a) = \sum_{s'} P(s'|s,a)[R(s,a,s') + \gamma \sum_{a'} \pi(a'|s') Q^\pi(s',a')]$。

Q: 写出贝尔曼最优方程（V*）。
A: $V^*(s) = \max_a \sum_{s'} P(s'|s,a)[R(s,a,s') + \gamma V^*(s')]$，[最优值函数]满足的条件。

Q: 最优值函数和最优策略为什么等价？
A: 最优策略 $\pi^*(s) = \arg\max_a Q^*(s,a)$，即在每个状态选择使 Q 最大的动作。知道 $Q^*$ 就能得到 $\pi^*$。

---

## 二、基本算法

### 2.1 动态规划 (DP)

Q: 动态规划求解 MDP 的前提条件？
A: 需要知道完整的[环境模型]（转移概率 P 和奖励函数 R），即 Model-based 方法。

Q: 策略迭代的两个步骤？
A: ①[策略评估]：计算当前策略的值函数 V；②[策略改进]：贪心更新策略 $\pi(s) = \arg\max_a Q(s,a)$。交替进行直到收敛。

Q: 值迭代的核心思想？
A: 直接迭代[贝尔曼最优方程]：$V(s) \leftarrow \max_a [R(s,a) + \gamma \sum_{s'} P(s'|s,a) V(s')]$，收敛后提取最优策略。

Q: 策略迭代和值迭代的区别？
A: [策略迭代]：每轮完整评估策略，迭代次数少但每次代价高。[值迭代]：每轮只更新一次值，迭代次数多但每次代价低。

### 2.2 蒙特卡洛方法 (MC)

Q: 蒙特卡洛方法的核心思想？
A: 通过[完整轨迹]的采样来估计值函数，$V(s) \approx \frac{1}{N}\sum_{i=1}^{N} G_i$，其中 $G_i$ 是第 i 次访问 s 后的回报。

Q: MC 方法的优缺点？
A: 优点：①不需要环境模型（[Model-free]）；②无偏估计。缺点：①方差高；②必须等到[回合结束]才能更新；③只适用于回合制任务。

Q: First-visit MC 和 Every-visit MC 的区别？
A: [First-visit]：每个回合只用第一次访问状态 s 的回报。[Every-visit]：使用所有访问的回报。两者都收敛到真实值。

### 2.3 时序差分 (TD)

Q: TD 学习的核心思想？
A: 用[自举]（Bootstrapping）估计：$V(s) \leftarrow V(s) + \alpha[r + \gamma V(s') - V(s)]$，不必等回合结束，每步都能更新。

Q: TD 误差的定义？
A: $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$，即[实际收获]与[预期]的差值。

Q: MC 和 TD 的主要区别？
A: [MC]：用完整回报 G，无偏但高方差，需回合结束。[TD]：用单步估计 r+γV(s')，有偏但低方差，[在线学习]。

Q: TD(λ) 中 λ 的作用？
A: λ 控制 MC 和 TD(0) 的[权衡]：λ=0 是纯 TD，λ=1 是纯 MC，0<λ<1 是多步回报的加权平均。

---

## 三、Value-Based 方法

### 3.1 Q-Learning

Q: Q-Learning 的更新公式？
A: $Q(s,a) \leftarrow Q(s,a) + \alpha[r + \gamma \max_{a'} Q(s',a') - Q(s,a)]$，使用[最大]动作值更新。

Q: Q-Learning 是 On-policy 还是 Off-policy？
A: [Off-policy]。行为策略（ε-greedy 探索）与目标策略（greedy）不同，用目标策略的 max Q 更新。

Q: Q-Learning 的收敛条件？
A: ①每个状态-动作对被[无限次]访问；②学习率满足 $\sum \alpha = \infty$，$\sum \alpha^2 < \infty$；③MDP 有限。

### 3.2 SARSA

Q: SARSA 的更新公式？
A: $Q(s,a) \leftarrow Q(s,a) + \alpha[r + \gamma Q(s',a') - Q(s,a)]$，使用[实际执行]的动作 a' 更新。

Q: SARSA 是 On-policy 还是 Off-policy？
A: [On-policy]。行为策略和目标策略相同（都是 ε-greedy），用实际采取的动作更新。

Q: Q-Learning 和 SARSA 的区别？
A: Q-Learning 更激进（用 max Q），学习最优策略但可能[不安全]。SARSA 更保守（用实际动作），考虑探索策略的影响。

Q: 在悬崖行走问题中，Q-Learning 和 SARSA 表现有何不同？
A: [SARSA] 会学习更安全的远离悬崖的路径，因为它考虑了探索时可能掉落。[Q-Learning] 学习最短路径（沿悬崖边）。

### 3.3 DQN

Q: DQN 的两个关键创新？
A: ①[经验回放]（Experience Replay）：打破数据相关性；②[目标网络]（Target Network）：稳定训练目标。

Q: 经验回放的作用？
A: ①打破样本的[时间相关性]，使数据更接近 i.i.d.；②重复利用历史经验，提高[样本效率]；③平滑数据分布变化。

Q: 目标网络为什么能稳定训练？
A: Q 网络同时用于生成目标和更新，导致[目标不稳定]。目标网络参数固定一段时间，减少更新震荡。

Q: DQN 的损失函数？
A: $L = \mathbb{E}[(r + \gamma \max_{a'} Q(s',a';\theta^-) - Q(s,a;\theta))^2]$，$\theta^-$ 是[目标网络]参数。

### 3.4 DQN 变体

Q: Double DQN 解决什么问题？
A: 解决 DQN 的[过估计]问题。用在线网络选动作，用目标网络评估：$r + \gamma Q(s', \arg\max_{a'} Q(s',a';\theta);\theta^-)$。

Q: Dueling DQN 的网络结构？
A: 将 Q 分解为[状态值 V] 和[优势函数 A]：$Q(s,a) = V(s) + A(s,a) - \frac{1}{|A|}\sum_{a'} A(s,a')$，更好地学习状态值。

Q: Prioritized Experience Replay 的思想？
A: 按 [TD 误差]确定优先级，优先采样误差大的样本（学习价值高），并用[重要性采样]修正偏差。

Q: Rainbow DQN 包含哪些改进？
A: Double DQN + [Dueling] + Prioritized Replay + [Multi-step] + [Distributional] + [Noisy Nets]，六合一。

---

## 四、Policy-Based 方法

### 4.1 策略梯度

Q: 策略梯度的核心思想？
A: 直接参数化策略 $\pi_\theta(a|s)$，通过梯度上升最大化[期望回报]：$\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)$。

Q: 策略梯度定理的公式？
A: $\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(a|s) \cdot Q^{\pi}(s,a)]$，[对数概率梯度]乘以动作价值。

Q: REINFORCE 算法的更新公式？
A: $\theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot G_t$，用蒙特卡洛回报 $G_t$ 估计 Q。

Q: REINFORCE 的问题？
A: [高方差]。回报 $G_t$ 波动大，导致梯度估计不稳定，收敛慢。

Q: 如何降低策略梯度的方差？
A: ①引入[基线] Baseline：$\nabla J = \mathbb{E}[\nabla \log \pi (Q - b)]$，b 不改变期望但减小方差；②使用 Actor-Critic。

### 4.2 Value-Based vs Policy-Based

Q: Value-Based 方法的优缺点？
A: 优点：[样本效率]高（off-policy）。缺点：只能处理离散动作；难以学习[随机策略]；不够稳定。

Q: Policy-Based 方法的优缺点？
A: 优点：①可处理[连续动作]；②可学习随机策略；③更好的收敛性。缺点：样本效率低；高方差；容易陷入局部最优。

---

## 五、Actor-Critic 方法

### 5.1 基本原理

Q: Actor-Critic 的基本思想？
A: [Actor]：策略网络，输出动作概率。[Critic]：值网络，评估动作好坏。Critic 的评估指导 Actor 更新。

Q: 优势函数 A(s,a) 的定义？
A: $A(s,a) = Q(s,a) - V(s)$，表示动作 a 相对于平均水平的[相对优势]，减小方差同时保持无偏。

Q: A2C 的更新方式？
A: Actor 用[优势函数]更新：$\nabla_\theta J = \mathbb{E}[\nabla \log \pi_\theta A(s,a)]$。Critic 用 TD 误差更新。

Q: A3C 的特点？
A: [异步]多进程训练：多个 worker 并行与环境交互，各自计算梯度后更新全局参数，加速训练且增加多样性。

### 5.2 PPO

Q: PPO 解决什么问题？
A: 解决策略梯度更新步长难以控制的问题，通过[裁剪]限制策略更新幅度，保持训练稳定。

Q: PPO-Clip 的目标函数？
A: $L^{CLIP} = \mathbb{E}[\min(r_t(\theta)A_t, clip(r_t(\theta), 1-\epsilon, 1+\epsilon)A_t)]$，$r_t = \frac{\pi_\theta}{\pi_{\theta_{old}}}$。

Q: PPO 中的 clip 操作的作用？
A: 当 $r_t$ 偏离 1 太多时裁剪，防止策略变化过大。$\epsilon$ 通常取 [0.1~0.2]，控制更新幅度。

Q: PPO 为什么广泛使用？
A: ①实现简单；②[超参数鲁棒]，调参容易；③性能稳定；④支持并行训练。是目前最常用的策略梯度算法。

### 5.3 TRPO

Q: TRPO 的核心思想？
A: 通过[信任域]约束保证每次更新后策略单调改进：$\max L(\theta)$，s.t. $D_{KL}(\pi_{\theta_{old}} || \pi_\theta) \leq \delta$。

Q: TRPO 如何保证策略单调改进？
A: 理论证明：在 KL 散度约束下，新策略的性能有[下界保证]，即 $J(\pi_{new}) \geq L(\pi_{old})$ - penalty。

Q: TRPO 和 PPO 的区别？
A: [TRPO]：硬约束（KL 散度），需要二阶优化（共轭梯度），计算复杂。[PPO]：软约束（裁剪），一阶优化，更简单高效。

---

## 六、连续动作空间

### 6.1 DDPG

Q: DDPG 的核心思想？
A: 将 DQN 扩展到[连续动作]空间：Actor 输出确定性动作 $a = \mu_\theta(s)$，Critic 评估 $Q(s,a)$。

Q: DDPG 中的"确定性"是什么意思？
A: Actor 输出确定的动作值（而非概率分布），用于连续空间。探索通过加[噪声]实现（如 OU 过程）。

Q: DDPG 的四个网络？
A: ①Actor 网络 $\mu_\theta$；②Critic 网络 $Q_\phi$；③[Target Actor] $\mu_{\theta'}$；④[Target Critic] $Q_{\phi'}$。

Q: DDPG 的 Actor 更新方式？
A: $\nabla_\theta J = \mathbb{E}[\nabla_a Q(s,a)|_{a=\mu(s)} \cdot \nabla_\theta \mu(s)]$，[链式法则]通过 Q 网络传梯度。

Q: DDPG 目标网络的软更新？
A: $\theta' \leftarrow \tau\theta + (1-\tau)\theta'$，$\tau$ 很小（如 0.001），[平滑更新]目标网络，比硬更新更稳定。

### 6.2 TD3

Q: TD3 对 DDPG 的三个改进？
A: ①[双 Q 网络]取较小值（减少过估计）；②[延迟更新] Actor（Critic 更新多次后再更新 Actor）；③[目标策略平滑]（加噪声）。

Q: TD3 中双 Q 网络的作用？
A: $y = r + \gamma \min(Q_1', Q_2')$，取两个 Critic 中较小的值，减少 [Q 值过估计]。

Q: TD3 目标策略平滑是什么？
A: 在目标动作上加裁剪噪声：$a' = \mu'(s') + clip(\epsilon, -c, c)$，使 Q 值估计更[平滑]，减少利用 Q 函数误差。

### 6.3 SAC

Q: SAC 的核心创新？
A: 最大化[熵正则化]的期望回报：$J = \mathbb{E}[\sum r_t + \alpha H(\pi(\cdot|s_t))]$，鼓励探索和鲁棒性。

Q: SAC 中熵正则化的好处？
A: ①鼓励[探索]；②学习多模态策略；③对超参数更鲁棒；④训练更稳定。

Q: SAC 的温度参数 α 的作用？
A: 控制[熵和奖励]的权衡。α 大→更随机探索；α 小→更确定性。SAC 可以自动调节 α。

Q: SAC 和 TD3 的主要区别？
A: [SAC]：随机策略 + 熵正则化 + 自动温度调节，探索更好。[TD3]：确定性策略 + 手动噪声，实现更简单。

---

## 七、进阶话题

### 7.1 Model-Based RL

Q: Model-Based 和 Model-Free 的区别？
A: [Model-Based]：学习/利用环境模型（转移+奖励），可规划，样本效率高。[Model-Free]：直接学策略/值，不建模环境，更简单但样本效率低。

Q: Model-Based 的优缺点？
A: 优点：[样本效率]高，可规划。缺点：模型误差会[累积]，复杂环境建模困难，计算开销大。

Q: Dyna-Q 的思想？
A: 结合 Model-Based 和 Model-Free：用真实经验更新值函数 + 学习模型 + 用模型[模拟经验]更新值函数。

### 7.2 多智能体 RL

Q: 多智能体 RL 的主要挑战？
A: ①环境[非平稳]（其他智能体也在学习）；②信用分配问题；③通信与协作；④联合动作空间指数增长。

Q: MARL 的三种设置？
A: ①[合作式]：共同最大化团队奖励；②[竞争式]：零和博弈；③[混合式]：既有合作又有竞争。

### 7.3 Offline RL

Q: Offline RL 的核心挑战？
A: 只能使用固定数据集，不能与环境交互。主要问题：[分布偏移]——策略可能选择数据中未见过的动作，导致 Q 值估计错误。

Q: 解决 Offline RL 分布偏移的方法？
A: ①[保守 Q 学习]（CQL）：惩罚未见动作的 Q 值；②行为克隆正则化；③隐式策略约束。

### 7.4 探索策略

Q: 常用的探索策略？
A: ①[ε-greedy]：概率 ε 随机，1-ε 贪心；②[Boltzmann]：按 Q 值 softmax 采样；③[UCB]：上置信界；④[好奇心驱动]探索。

Q: 什么是内在奖励（Intrinsic Reward）？
A: 除环境奖励外的[内部激励]，如好奇心（预测误差）、新颖性（访问次数）、信息增益，用于鼓励探索稀疏奖励环境。

---

## 八、实际应用

### 8.1 RLHF

Q: RLHF 的基本流程？
A: ①[监督微调]（SFT）；②训练[奖励模型]（人类偏好）；③用 [PPO] 优化策略使奖励最大化。

Q: RLHF 中为什么用 PPO？
A: ①稳定，不会偏离参考模型太远；②可以加 [KL 惩罚]约束与 SFT 模型的距离；③超参数鲁棒。

Q: DPO 相比 RLHF 的优势？
A: [直接偏好优化]：无需训练奖励模型，直接用偏好数据优化策略，更简单高效，稳定性更好。

### 8.2 游戏与机器人

Q: AlphaGo 使用了哪些 RL 技术？
A: ①[蒙特卡洛树搜索]（MCTS）；②策略网络（模仿学习 + 自我对弈强化学习）；③值网络（评估局面）。

Q: 机器人控制常用什么 RL 算法？
A: [SAC] 和 [TD3]（连续动作空间）最常用。有时使用 Model-Based 方法提高样本效率。

### 8.3 推荐系统

Q: RL 用于推荐系统的优势？
A: ①考虑[长期收益]而非即时点击；②处理用户兴趣[动态变化]；③优化序列推荐策略。

Q: 推荐系统中 RL 的挑战？
A: ①[状态空间]巨大（用户历史）；②奖励稀疏且延迟；③探索代价高（影响用户体验）；④离线评估困难。

---

*Generated by hashcards format for spaced repetition learning*