# 📕 深度学习面试 Hashcards

> 按照 hashcards 格式设计，使用 `Q:` 和 `A:` 标记问答，`[关键词]` 用于挖空记忆

---

## 一、基础概念

### 1.1 训练相关术语

Q: Epoch、Iteration、Batch Size 的关系？
A: [Batch Size]：每次迭代使用的样本数；[Iteration]：一次前向+反向传播；[Epoch]：遍历完整个训练集。关系：1 epoch = 样本数 / batch_size 次 iteration。

Q: 为什么使用 Mini-Batch 而非全量数据？
A: ①内存限制；②随机性有助于跳出[局部最优]；③更频繁的参数更新加速收敛；④GPU 并行计算效率。

Q: Batch Size 大小如何影响训练？
A: 大 Batch：梯度估计更准，收敛更稳定，但可能陷入[尖锐极小值]，泛化差。小 Batch：噪声大，有正则化效果，更可能找到[平坦极小值]。

### 1.2 反向传播

Q: 反向传播的核心思想？
A: 利用[链式法则]从输出层到输入层逐层计算损失函数对各参数的梯度，实现高效的梯度计算。

Q: 梯度消失和梯度爆炸的原因？
A: 深层网络中梯度连乘。若每层梯度 <1，连乘后[趋近于0]（消失）；若 >1，连乘后[趋近于∞]（爆炸）。

Q: 解决梯度消失/爆炸的方法？
A: ①使用 [ReLU] 等激活函数；②[BatchNorm]；③[残差连接]；④梯度裁剪；⑤合适的初始化（Xavier/He）；⑥LSTM/GRU。

---

## 二、激活函数

### 2.1 常用激活函数

Q: Sigmoid 函数的问题？
A: ①输出不以0为中心，影响收敛；②两端[梯度饱和]，易梯度消失；③指数运算计算量大。

Q: ReLU 的优缺点？
A: 优点：①计算简单；②缓解[梯度消失]；③稀疏激活。缺点：①输出不以0为中心；②[Dead ReLU]：负区间梯度为0，神经元可能"死亡"。

Q: Leaky ReLU 和 PReLU 的区别？
A: [Leaky ReLU]：负区间斜率固定（如0.01）。[PReLU]：负区间斜率 α 作为可学习参数。

Q: GELU 激活函数的特点？
A: $GELU(x) = x \cdot \Phi(x)$，其中 Φ 是标准正态 CDF。特点：[平滑]非单调，在 Transformer 中广泛使用，结合了 dropout 的思想。

Q: Swish/SiLU 激活函数？
A: $Swish(x) = x \cdot \sigma(\beta x)$，当 β=1 时即 [SiLU]。特点：平滑、非单调、自门控，在 EfficientNet、Transformer 中表现好。

### 2.2 Softmax

Q: Softmax 函数的作用？
A: 将 K 维向量转换为[概率分布]：$softmax(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}$，输出和为1，常用于多分类最后一层。

Q: Softmax 的数值稳定性问题如何解决？
A: 减去最大值：$softmax(z_i) = \frac{e^{z_i - max(z)}}{\sum_j e^{z_j - max(z)}}$，防止指数[溢出]。

---

## 三、卷积神经网络 (CNN)

### 3.1 卷积基础

Q: 卷积的核心优势？
A: ①[局部连接]：提取局部特征；②[权值共享]：大幅减少参数；③[平移不变性]；④层级特征学习。

Q: 卷积输出尺寸计算公式？
A: $O = \lfloor\frac{W - K + 2P}{S}\rfloor + 1$，其中 W=输入尺寸，K=[卷积核]大小，P=[Padding]，S=[Stride]。

Q: 1×1 卷积的作用？
A: ①[通道变换]（升维/降维）；②跨通道信息融合；③引入非线性；④降低计算量（用于 Bottleneck）。

Q: 为什么通常使用奇数卷积核（3×3, 5×5）？
A: ①有明确的[中心点]，便于定位；②对称的 padding 保持输出尺寸；③3×3 是最小的能捕获上下左右中心信息的核。

### 3.2 池化层

Q: 池化的作用？
A: ①[降维]减少参数；②增大感受野；③提供[平移不变性]；④防止过拟合。

Q: Max Pooling 和 Average Pooling 的区别？
A: [Max Pooling]：保留最显著特征，反向传播时梯度只传给最大值位置。[Average Pooling]：保留全局信息，梯度均分给所有位置。

Q: Global Average Pooling (GAP) 的优势？
A: 将每个特征图取平均得到一个值，[替代全连接层]减少参数，防止过拟合，常用于分类网络最后。

### 3.3 感受野

Q: 感受野的定义？
A: 输出特征图上一个像素对应原始输入图像的[区域大小]，决定了网络能"看到"多大范围的信息。

Q: 感受野的计算公式？
A: $RF_{l+1} = RF_l + (K-1) \times \prod_{i=1}^{l} S_i$，或递推：$RF_{l+1} = (RF_l - 1) \times S + K$。

Q: 如何增大感受野？
A: ①加深网络；②使用更大的卷积核；③增大[步长]；④使用[空洞卷积]（Dilated Convolution）。

### 3.4 特殊卷积

Q: 深度可分离卷积的组成？
A: [Depthwise Conv]：每个输入通道用单独卷积核 + [Pointwise Conv]：1×1 卷积融合通道。参数量从 $K^2 \cdot C_{in} \cdot C_{out}$ 降到 $K^2 \cdot C_{in} + C_{in} \cdot C_{out}$。

Q: 空洞卷积（Dilated Convolution）的作用？
A: 在卷积核元素间插入空洞（dilation rate），[不增加参数]的情况下扩大感受野，常用于语义分割（DeepLab）。

Q: 转置卷积（Transposed Convolution）是什么？
A: 也叫反卷积/分数步长卷积，是一种[上采样]操作。注意：不是卷积的逆运算，只能恢复尺寸不能恢复数值。

Q: 分组卷积（Group Convolution）的思想？
A: 将输入通道分成 G 组，每组独立卷积后拼接。参数量和计算量降为 [1/G]，AlexNet 和 ResNeXt 使用。

---

## 四、经典网络架构

### 4.1 VGG / ResNet

Q: VGG 的核心设计思想？
A: 使用[堆叠的 3×3 卷积]代替大卷积核：两个 3×3 等效于 5×5，三个等效于 7×7，参数更少且增加非线性。

Q: ResNet 解决了什么问题？
A: 深层网络的[退化问题]（不是过拟合）：网络越深，训练误差反而上升。通过残差连接让网络学习 F(x) = H(x) - x。

Q: 残差连接为什么有效？
A: ①梯度可以[直接回传]，缓解梯度消失；②学习残差比学习完整映射更容易；③实现了[恒等映射]的 shortcut。

Q: ResNet 中 1×1 卷积的 Bottleneck 设计？
A: 1×1 降维 → 3×3 卷积 → 1×1 升维。[减少 3×3 的计算量]，同时保持通道数，用于深层 ResNet（50/101/152）。

### 4.2 轻量级网络

Q: MobileNet V1 的核心创新？
A: [深度可分离卷积]：将标准卷积分解为 Depthwise + Pointwise，计算量降低约 8-9 倍。

Q: MobileNet V2 的改进？
A: [倒残差结构]（Inverted Residual）：先升维（1×1）→ Depthwise → 再降维（1×1），且残差连接在[窄层]之间。

Q: ShuffleNet 的核心操作？
A: [Channel Shuffle]：解决分组卷积中组间信息不流通的问题，通过重排通道实现跨组信息交换。

### 4.3 注意力机制

Q: SENet 的核心思想？
A: [通道注意力]：Squeeze（GAP）→ Excitation（两层 FC）→ Scale，学习各通道的重要性权重。

Q: CBAM 包含哪两个模块？
A: [通道注意力]（类似 SE）+ [空间注意力]（Max+Avg → Conv → Sigmoid），串联使用。

---

## 五、归一化方法

### 5.1 Batch Normalization

Q: BN 的计算过程？
A: ①计算 mini-batch 的[均值和方差]；②归一化：$(x-\mu)/\sqrt{\sigma^2+\epsilon}$；③缩放平移：$\gamma x + \beta$（可学习参数）。

Q: BN 为什么有效？（目前理解）
A: ①使损失函数更[平滑]（Lipschitz 性质更好）；②解耦方向和长度的优化；③允许使用更大学习率。注：原论文的 ICS 解释已被质疑。

Q: BN 在训练和推理时的区别？
A: 训练：使用当前 batch 的统计量。推理：使用训练时累积的[移动平均]（running mean/var），保证确定性输出。

Q: BN 不适用于什么场景？
A: ①[小 batch size]（统计量不准）；②序列长度变化的 [RNN]；③在线学习（单样本）。

### 5.2 其他归一化

Q: Layer Normalization 的特点？
A: 对单个样本的[所有特征]做归一化（而非跨 batch），不依赖 batch size，适合 [RNN/Transformer]。

Q: Instance Normalization 用在哪？
A: 对单个样本的[单个通道]做归一化，常用于[风格迁移]（去除风格信息）。

Q: Group Normalization 的设计？
A: 将通道分成若干组，对每个样本的[每组通道]做归一化。介于 LN 和 IN 之间，不受 batch size 限制。

Q: BN、LN、IN、GN 的归一化维度？
A: [BN]：(N,H,W)；[LN]：(C,H,W)；[IN]：(H,W)；[GN]：(C/G,H,W)。N=batch, C=channel, H,W=空间。

---

## 六、优化器

### 6.1 基础优化器

Q: SGD 的更新公式？
A: $\theta_{t+1} = \theta_t - \eta \nabla L(\theta_t)$，基础的[随机梯度下降]，收敛速度慢，对学习率敏感。

Q: Momentum 的作用？
A: 引入[动量项]累积历史梯度：$v_t = \gamma v_{t-1} + \eta \nabla L$，$\theta_{t+1} = \theta_t - v_t$。加速收敛，减少震荡。

Q: AdaGrad 的特点和问题？
A: 对每个参数[自适应学习率]，历史梯度大的参数学习率小。问题：学习率[单调递减]，后期可能过小。

Q: RMSprop 如何改进 AdaGrad？
A: 用[指数移动平均]代替梯度平方的累加：$E[g^2]_t = \rho E[g^2]_{t-1} + (1-\rho)g_t^2$，避免学习率过快下降。

### 6.2 Adam 及变体

Q: Adam 的核心思想？
A: 结合 [Momentum]（一阶矩估计）和 [RMSprop]（二阶矩估计），自适应学习率。$m_t, v_t$ 分别估计梯度的均值和方差。

Q: Adam 的偏差修正是什么？
A: 初期 $m_t, v_t$ 偏向0（初始化为0），需修正：$\hat{m}_t = m_t/(1-\beta_1^t)$，$\hat{v}_t = v_t/(1-\beta_2^t)$。

Q: AdamW 相比 Adam 的改进？
A: Adam 中 L2 正则与自适应学习率耦合。[AdamW] 将权重衰减[解耦]出来：$\theta = \theta - \eta(\hat{m}/\sqrt{\hat{v}} + \lambda\theta)$，效果更好。

Q: 常用的 Adam 超参数？
A: $\beta_1=0.9$（一阶矩），$\beta_2=0.999$（二阶矩），$\epsilon=10^{-8}$，学习率 $\eta$ 通常 $10^{-3}$ 或 $10^{-4}$。

---

## 七、正则化技术

### 7.1 Dropout

Q: Dropout 的原理？
A: 训练时以概率 p [随机丢弃]神经元（输出置0），测试时保留全部但权重乘以 (1-p)。

Q: Dropout 为什么有效？
A: ①减少神经元[共适应]；②近似[模型集成]；③引入噪声，提供正则化。

Q: Dropout 在训练和推理时的区别？
A: 训练：随机丢弃，输出除以 (1-p)（Inverted Dropout）。推理：[不丢弃]，保持期望一致。

Q: Dropout 和 BN 能否同时使用？
A: 需谨慎。BN 依赖 batch 统计量，Dropout 改变了[激活分布]，可能导致不一致。通常建议：BN 在 Dropout 之后，或只用其一。

### 7.2 数据增强

Q: 图像常用的数据增强方法？
A: 几何变换（翻转、旋转、裁剪、缩放）、颜色变换（亮度、对比度、饱和度）、[Mixup]、[CutMix]、[AutoAugment]。

Q: Mixup 的原理？
A: 对两个样本 $(x_i, y_i), (x_j, y_j)$ 线性插值：$\tilde{x} = \lambda x_i + (1-\lambda)x_j$，$\tilde{y} = \lambda y_i + (1-\lambda)y_j$，[软标签]训练。

Q: CutMix 与 Mixup 的区别？
A: CutMix 是[区域级别]的混合：从一张图切一块贴到另一张，标签按面积比例混合，保留更多局部信息。

---

## 八、损失函数

### 8.1 分类损失

Q: 交叉熵损失的公式？
A: 二分类：$L = -[y\log(\hat{y}) + (1-y)\log(1-\hat{y})]$。多分类：$L = -\sum_c y_c \log(\hat{y}_c)$。

Q: Focal Loss 解决什么问题？
A: 解决[类别不均衡]问题：$FL = -\alpha(1-p_t)^\gamma \log(p_t)$。难样本权重大，易样本权重小。

Q: Label Smoothing 的作用？
A: 将 one-hot 标签软化：$y = (1-\epsilon)y + \epsilon/K$，防止模型[过度自信]，提升泛化。

### 8.2 回归损失

Q: L1 Loss 和 L2 Loss 的区别？
A: [L1]（MAE）：对异常值鲁棒，梯度恒定。[L2]（MSE）：对大误差惩罚大，梯度随误差变化，更易优化。

Q: Smooth L1 Loss 的设计思想？
A: 结合 L1 和 L2 的优点：误差小时用 L2（平滑），误差大时用 L1（鲁棒）。常用于目标检测（如 Faster R-CNN）。

---

## 九、权重初始化

### 9.1 初始化方法

Q: 为什么不能全零初始化？
A: 所有神经元输出相同，梯度相同，参数更新相同，导致[对称性问题]，网络无法学习。

Q: Xavier 初始化的原理？
A: 使每层输出方差与输入相同，权重从 $\mathcal{N}(0, \frac{2}{n_{in}+n_{out}})$ 采样，适合 [Tanh/Sigmoid]。

Q: He 初始化（Kaiming）的原理？
A: 考虑 ReLU 的特性（负半轴为0），权重从 $\mathcal{N}(0, \frac{2}{n_{in}})$ 采样，适合 [ReLU] 系列。

Q: 初始化和激活函数如何匹配？
A: Xavier → Tanh/Sigmoid/Softmax；He → [ReLU/Leaky ReLU]；SELU → LeCun 初始化。

---

## 十、学习率调度

Q: 常用的学习率调度策略？
A: ①[Step Decay]：固定步数衰减；②[Exponential]：指数衰减；③[Cosine Annealing]：余弦退火；④[Warmup]：从小学习率逐渐增大。

Q: Warmup 的作用？
A: 训练初期参数不稳定，用小学习率[预热]避免震荡，再逐渐增大。常用于 Transformer、大 batch 训练。

Q: Cosine Annealing 的特点？
A: 学习率按余弦函数从初始值下降到接近0，比阶梯下降更[平滑]，后期给模型更精细调整的机会。

---

## 十一、经典问题

### 11.1 模型调试

Q: 训练损失下降但验证损失不降，什么原因？
A: [过拟合]。解决：增加数据、数据增强、增强正则化（Dropout/L2/Early Stopping）、简化模型。

Q: 训练损失不下降，可能原因？
A: ①[学习率]过大或过小；②梯度消失/爆炸；③数据问题（标签错误、未 shuffle）；④模型太简单（欠拟合）；⑤Bug。

Q: 深度学习调参的一般顺序？
A: ①学习率（最重要）；②Batch Size；③网络结构（深度、宽度）；④正则化强度；⑤优化器选择。

### 11.2 网络设计

Q: 网络深度和宽度的作用？
A: [深度]：学习更抽象的特征表示，表达能力指数增长。[宽度]：每层学习更多特征，但增加计算量。

Q: 3×3 卷积堆叠 vs 大卷积核？
A: 两个 3×3 ≈ 一个 5×5 感受野，但参数 $2 \times 3^2 < 5^2$，且多一层[非线性]。

Q: 为什么使用预训练模型？
A: ①[迁移学习]：利用大数据集学到的通用特征；②加速收敛；③小数据集防止过拟合；④通常效果更好。

---

*Generated by hashcards format for spaced repetition learning*